{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column headers for Market 1:\n",
      "File: Market 1 Customers.json\n",
      "Customer ID\n",
      "Last Used Platform\n",
      "Is Blocked\n",
      "Created At\n",
      "Language\n",
      "Outstanding Amount\n",
      "Loyalty Points\n",
      "Number of employees\n",
      "----------------------------------------\n",
      "File: Market 1 Orders.csv\n",
      "Order ID\n",
      "Order Status\n",
      "Category Name\n",
      "SKU\n",
      "Customization Group\n",
      "Customization Option\n",
      "Quantity\n",
      "Unit Price\n",
      "Cost Price\n",
      "Total Cost Price\n",
      "Total Price\n",
      "Order Total\n",
      "Sub Total\n",
      "Tax\n",
      "Delivery Charge\n",
      "Tip\n",
      "Discount\n",
      "Remaining Balance\n",
      "Payment Method\n",
      "Additional Charge\n",
      "Taxable Amount\n",
      "Transaction ID\n",
      "Currency Symbol\n",
      "Transaction Status\n",
      "Promo Code\n",
      "Customer ID\n",
      "Merchant ID\n",
      "Description\n",
      "Distance (in km)\n",
      "Order Time\n",
      "Pickup Time\n",
      "Delivery Time\n",
      "Ratings\n",
      "Reviews\n",
      "Merchant Earning\n",
      "Commission Amount\n",
      "Commission Payout Status\n",
      "Order Preparation Time\n",
      "Debt Amount\n",
      "Redeemed Loyalty Points\n",
      "Consumed Loyalty Points\n",
      "Cancellation Reason\n",
      "Flat Discount\n",
      "Checkout Template Name\n",
      "Checkout Template Value\n",
      "----------------------------------------\n",
      "File: Market 1 Deliveries.csv\n",
      "Task_ID\n",
      "Order_ID\n",
      "Relationship\n",
      "Team_Name\n",
      "Task_Type\n",
      "Notes\n",
      "Agent_ID\n",
      "Agent_Name\n",
      "Distance(m)\n",
      "Total_Time_Taken(min)\n",
      "Task_Status\n",
      "Ref_Images\n",
      "Rating\n",
      "Review\n",
      "Latitude\n",
      "Longitude\n",
      "Tags\n",
      "Promo_Applied\n",
      "Custom_Template_ID\n",
      "Task_Details_QTY\n",
      "Task_Details_AMOUNT\n",
      "Special_Instructions\n",
      "Tip\n",
      "Delivery_Charges\n",
      "Discount\n",
      "Subtotal\n",
      "Payment_Type\n",
      "Task_Category\n",
      "Earning\n",
      "Pricing\n",
      "Unnamed: 30\n",
      "Unnamed: 31\n",
      "----------------------------------------\n",
      "\n",
      "Column headers for Market 2:\n",
      "File: Market 2 Customers.json\n",
      "Customer ID\n",
      "Last Used Platform\n",
      "Is Blocked\n",
      "Created At\n",
      "Language\n",
      "Outstanding Amount\n",
      "Loyalty Points\n",
      "Number of Employees\n",
      "----------------------------------------\n",
      "File: Market 2 Orders.csv\n",
      "Order ID\n",
      "Order Status\n",
      "Category Name\n",
      "SKU\n",
      "Customization Group\n",
      "Customization Option\n",
      "Quantity\n",
      "Unit Price\n",
      "Cost Price\n",
      "Total Cost Price\n",
      "Total Price\n",
      "Order Total\n",
      "Sub Total\n",
      "Tax\n",
      "Delivery Charge\n",
      "Tip\n",
      "Discount\n",
      "Remaining Balance\n",
      "Payment Method\n",
      "Additional Charge\n",
      "Taxable Amount\n",
      "Transaction ID\n",
      "Currency Symbol\n",
      "Transaction Status\n",
      "Promo Code\n",
      "Customer ID\n",
      "Merchant ID\n",
      "Description\n",
      "Distance (in km)\n",
      "Order Time\n",
      "Pickup Time\n",
      "Delivery Time\n",
      "Ratings\n",
      "Reviews\n",
      "Merchant Earning\n",
      "Commission Amount\n",
      "Commission Payout Status\n",
      "Order Preparation Time\n",
      "Redeemed Loyalty Points\n",
      "Consumed Loyalty Points\n",
      "Cancellation Reason\n",
      "Flat Discount\n",
      "Checkout Template Name\n",
      "Checkout Template Value\n",
      "----------------------------------------\n",
      "File: Market 2 Deliveries.csv\n",
      "Task_ID\n",
      "Order_ID\n",
      "Relationship\n",
      "Team_Name\n",
      "Task_Type\n",
      "Notes\n",
      "Agent_ID\n",
      "Distance(m)\n",
      "Total_Time_Taken(min)\n",
      "Task_Status\n",
      "Ref_Images\n",
      "Rating\n",
      "Review\n",
      "Latitude\n",
      "Longitude\n",
      "Tags\n",
      "Promo_Applied\n",
      "Custom_Template_ID\n",
      "Task_Details_QTY\n",
      "Task_Details_AMOUNT\n",
      "Special_Instructions\n",
      "Tip\n",
      "Delivery_Charges\n",
      "Discount\n",
      "Subtotal\n",
      "Payment_Type\n",
      "Task_Category\n",
      "Earning\n",
      "Pricing\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_names = [\n",
    "    \"Market 1 Customers.json\",\n",
    "    \"Market 2 Customers.json\",\n",
    "    \"Market 1 Orders.csv\",\n",
    "    \"Market 2 Orders.csv\",\n",
    "    \"Market 1 Deliveries.csv\",\n",
    "    \"Market 2 Deliveries.csv\"\n",
    "]\n",
    "\n",
    "market1_columns = {}\n",
    "market2_columns = {}\n",
    "\n",
    "# Read the column headers from each file and store them in the respective dictionaries\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(file_name)\n",
    "    try:\n",
    "        if file_name.lower().endswith(\".json\"):\n",
    "            df = pd.read_json(file_path)\n",
    "        elif file_name.lower().endswith(\".csv\"):\n",
    "            # Explicitly set low_memory=False to avoid DtypeWarning\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Store column headers in the respective dictionaries based on market\n",
    "        if \"Market 1\" in file_name:\n",
    "            market1_columns[file_name] = df.columns.tolist()\n",
    "        elif \"Market 2\" in file_name:\n",
    "            market2_columns[file_name] = df.columns.tolist()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "\n",
    "# Display the column headers for both markets\n",
    "print(\"Column headers for Market 1:\")\n",
    "for file_name, headers in market1_columns.items():\n",
    "    print(f\"File: {file_name}\")\n",
    "    for header in headers:\n",
    "        print(header)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nColumn headers for Market 2:\")\n",
    "for file_name, headers in market2_columns.items():\n",
    "    print(f\"File: {file_name}\")\n",
    "    for header in headers:\n",
    "        print(header)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column headers for both markets are stored in 'market_column_headers.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_names = [\n",
    "    \"Market 1 Customers.json\",\n",
    "    \"Market 2 Customers.json\",\n",
    "    \"Market 1 Orders.csv\",\n",
    "    \"Market 2 Orders.csv\",\n",
    "    \"Market 1 Deliveries.csv\",\n",
    "    \"Market 2 Deliveries.csv\"\n",
    "]\n",
    "\n",
    "market1_columns = []\n",
    "market2_columns = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(file_name)\n",
    "    try:\n",
    "        if file_name.lower().endswith(\".json\"):\n",
    "            df = pd.read_json(file_path)\n",
    "        elif file_name.lower().endswith(\".csv\"):\n",
    "            # Explicitly set low_memory=False to avoid DtypeWarning\n",
    "            df = pd.read_csv(file_path, low_memory=False)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Store column headers in the respective lists based on market\n",
    "        if \"Market 1\" in file_name:\n",
    "            market1_columns.append([file_name] + df.columns.tolist())\n",
    "        elif \"Market 2\" in file_name:\n",
    "            market2_columns.append([file_name] + df.columns.tolist())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "\n",
    "# Combine column headers for both markets\n",
    "combined_columns = market1_columns + market2_columns\n",
    "\n",
    "# Write the column headers to an Excel file\n",
    "with pd.ExcelWriter(\"market_column_headers.xlsx\") as writer:\n",
    "    pd.DataFrame(combined_columns).to_excel(writer, sheet_name='Market', index=False, header=False)\n",
    "\n",
    "print(\"Column headers for both markets are stored in 'market_column_headers.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined customer data is saved in 'combined_customer_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the list of file names for customer datasets\n",
    "customer_files = [\n",
    "    \"Market 1 Customers.json\",\n",
    "    \"Market 2 Customers.json\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dataframes for each customer dataset\n",
    "customer_dataframes = []\n",
    "\n",
    "# Read data from each customer dataset and append to the list\n",
    "for file_name in customer_files:\n",
    "    file_path = os.path.join(file_name)\n",
    "    try:\n",
    "        if file_name.lower().endswith(\".json\"):\n",
    "            df = pd.read_json(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {file_name}\")\n",
    "            continue\n",
    "        customer_dataframes.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found.\")\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "combined_customer_data = pd.concat(customer_dataframes, ignore_index=True)\n",
    "\n",
    "    \n",
    "# Save consolidated data to the output subfolder\n",
    "output_file = os.path.join(\"combined_customer_data.csv\")\n",
    "\n",
    "combined_customer_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined customer data is saved in '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'combined_customer_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Read the combined customer data from the CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_customer_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m combined_customer_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_file)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Copy values from \"Number of Employees\" to \"Number of employees\" and drop the original column\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Employees\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m combined_customer_data\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'combined_customer_data.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the combined customer data from the CSV file\n",
    "input_file = \"combined_customer_data.csv\"\n",
    "combined_customer_data = pd.read_csv(input_file)\n",
    "\n",
    "# Copy values from \"Number of Employees\" to \"Number of employees\" and drop the original column\n",
    "if \"Number of Employees\" in combined_customer_data.columns:\n",
    "    combined_customer_data[\"Number of employees\"] = combined_customer_data[\"Number of Employees\"]\n",
    "    combined_customer_data.drop(columns=[\"Number of Employees\"], inplace=True)\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = \"transformed_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file in the output folder\n",
    "output_file = os.path.join(output_folder, \"customer_data.csv\")\n",
    "combined_customer_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Modified customer data is saved in '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset with standardized column names is saved in 'transformed_data\\orders.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    \"\"\"Standardizes column names in a DataFrame.\"\"\"\n",
    "    standardized_names = {}\n",
    "\n",
    "    for original_column in df.columns:\n",
    "        new_column = original_column.lower().replace(\" \", \"_\")\n",
    "        standardized_names[new_column] = original_column\n",
    "\n",
    "    df.rename(columns=standardized_names, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Read both datasets\n",
    "market1_orders = pd.read_csv(\"Market 1 Orders.csv\")\n",
    "market2_orders = pd.read_csv(\"Market 2 Orders.csv\")\n",
    "\n",
    "# Standardize column names\n",
    "market1_orders = standardize_column_names(market1_orders)\n",
    "market2_orders = standardize_column_names(market2_orders)\n",
    "\n",
    "# Check for missing columns in each dataset\n",
    "missing_columns_market1 = set(market2_orders.columns) - set(market1_orders.columns)\n",
    "missing_columns_market2 = set(market1_orders.columns) - set(market2_orders.columns)\n",
    "\n",
    "# Add missing columns with null values and ensure data consistency\n",
    "for column in missing_columns_market1:\n",
    "    market1_orders[column] = pd.NA\n",
    "\n",
    "for column in missing_columns_market2:\n",
    "    market2_orders[column] = pd.NA\n",
    "\n",
    "# Merge the datasets\n",
    "merged_orders = pd.concat([market1_orders, market2_orders], ignore_index=True)\n",
    "\n",
    "# Replace \"-\" values with NaN\n",
    "merged_orders.replace(\"-\", pd.NA, inplace=True)\n",
    "\n",
    "# Drop columns with empty/null rows or containing only \"-\" values\n",
    "def drop_empty_or_dash_columns(df):\n",
    "    return df.dropna(axis=1, how='all') \\\n",
    "             .loc[:, (df != '-').any()]  # Select columns where any value is not '-'\n",
    "\n",
    "merged_orders = drop_empty_or_dash_columns(merged_orders.copy())  # Avoid modifying original\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = \"transformed_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Write the merged dataset to a new CSV file in the output folder\n",
    "merged_file_path = os.path.join(output_folder, \"orders.csv\")\n",
    "merged_orders.to_csv(merged_file_path, index=False)\n",
    "\n",
    "print(f\"Merged dataset with standardized column names is saved in '{merged_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data with standardized columns saved to 'combined_deliveries.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_standardized_column_names(file_names):\n",
    "    \"\"\"Gets standardized column names from the provided files.\"\"\"\n",
    "    standardized_columns = {}\n",
    "\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "\n",
    "        for original_column in df.columns:\n",
    "            # Example standardization logic (customize this)\n",
    "            new_column = original_column.lower().replace(\" \", \"_\")\n",
    "\n",
    "            # Handle duplicates (e.g., suffix market number)\n",
    "            if new_column in standardized_columns:\n",
    "                new_column += \"_market_\" + file_name.split(\" \")[0][-1] \n",
    "            standardized_columns[new_column] = original_column\n",
    "\n",
    "    return standardized_columns\n",
    "\n",
    "def combine_and_save_data(file_names, standardized_columns):\n",
    "    \"\"\"Combines data with standardized names and saves to a CSV file.\"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        df = pd.read_csv(file_name, low_memory=False)\n",
    "        df.rename(columns=standardized_columns, inplace=True)\n",
    "        all_data.append(df)\n",
    "\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df.to_csv(\"combined_deliveries.csv\", index=False) \n",
    "# Main Execution\n",
    "file_names = [\"Market 1 Deliveries.csv\", \"Market 2 Deliveries.csv\"]\n",
    "\n",
    "standardized_columns = get_standardized_column_names(file_names)\n",
    "combine_and_save_data(file_names, standardized_columns)\n",
    "\n",
    "print(\"Combined data with standardized columns saved to 'combined_deliveries.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\2103183778.py:79: DtypeWarning: Columns (7,9,12,14,16,19,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"combined_deliveries.csv\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def update_data(df):\n",
    "    \"\"\"\n",
    "    Concatenates 'Agent ID' with 'Notes' and moves 'Agent Number' data to 'Agent ID'.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to modify.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    # Concatenate Agent ID and Notes\n",
    "    df['Notes'] = df['Agent_ID'].astype(str) + ': ' + df['Notes']\n",
    "\n",
    "    df['Agent_ID'] = df['Agent_Name'] \n",
    "\n",
    "    df['Agent_Name'] = df['Distance(m)'] \n",
    "\n",
    "    df['Distance(m)'] = df['Total_Time_Taken(min)']\n",
    "\n",
    "    df['Total_Time_Taken(min)'] = df['Task_Status'] \n",
    "\n",
    "    df['Task_Status'] = df['Ref_Images'] \n",
    "\n",
    "    df['Ref_Images'] = df['Rating'] \n",
    "\n",
    "    df['Rating'] = df['Review'] \n",
    "\n",
    "    df['Review'] = df['Latitude'] \n",
    "\n",
    "    df['Latitude'] = df['Longitude'] \n",
    "\n",
    "    df['Longitude'] = df['Tags']\n",
    "\n",
    "    df['Tags'] = df['Promo_Applied'] \n",
    "\n",
    "    df['Promo_Applied'] = df['Custom_Template_ID']\n",
    "\n",
    "    df['Custom_Template_ID'] = df['Task_Details_QTY']\n",
    "\n",
    "    df['Task_Details_QTY'] = df['Task_Details_AMOUNT']\n",
    "\n",
    "    df['Task_Details_AMOUNT'] = df['Special_Instructions']\n",
    "\n",
    "    df['Special_Instructions'] = ''\n",
    "\n",
    "    df['Special_Instructions'] = df['Tip']\n",
    "\n",
    "    # Concatenate Tip with the existing Special_Instructions\n",
    "    df['Special_Instructions'] = df['Special_Instructions'].astype(str) + ': ' + df['Tip'] \n",
    "\n",
    "\n",
    "    # Clean currency symbols\n",
    "    df['Tip'] = df['Tip'].str.replace('KSh ', '')\n",
    "\n",
    "    # Fill missing 'Tip' with 'KSh 0.00' \n",
    "    df['Tip'] = df['Tip'].fillna('KSh 0.00')\n",
    "\n",
    "\n",
    "    df['Tip'] = df['Delivery_Charges']\n",
    "\n",
    "    df['Delivery_Charges'] = df['Discount']\n",
    "\n",
    "    df['Discount'] = df['Subtotal']\n",
    "\n",
    "    df['Subtotal'] = df['Payment_Type']\n",
    "\n",
    "    df['Payment_Type'] = df['Task_Category']\n",
    "\n",
    "    df['Task_Category'] = df['Earning']\n",
    "\n",
    "    df['Earning'] = df['Pricing']\n",
    "\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"combined_deliveries.csv\")\n",
    "\n",
    "# Update the DataFrame\n",
    "updated_df = update_data(df.copy())  # Operate on a copy\n",
    "\n",
    "updated_df.to_csv(\"updated_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\4254366319.py:57: DtypeWarning: Columns (6,8,11,13,15,18,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"updated_data.csv\")\n",
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\4254366319.py:49: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('-', np.nan, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved in 'transformed_data\\deliveries.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def update_data(df):\n",
    "    \"\"\"\n",
    "    Concatenates 'Agent ID' with 'Notes' and moves 'Agent Number' data to 'Agent ID'.\n",
    "    Performs necessary string replacements in the 'Order_ID' column.\n",
    "    Replaces \"-\" values with NaN in each column.\n",
    "    Drops 'Unnamed: 30' and 'Unnamed: 31' columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to modify.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The updated DataFrame.\n",
    "    \"\"\"\n",
    "    # Concatenate Tip with the existing Special_Instructions\n",
    "    df['Special_Instructions'] = df['Special_Instructions'].astype(str) + ': ' + df['Tip'] \n",
    "    \n",
    "    df['Special_Instructions'] = ''\n",
    "\n",
    "    df['Special_Instructions'] = df['Tip']\n",
    "\n",
    "    # Clean currency symbols\n",
    "    df['Tip'] = df['Tip'].str.replace('KSh ', '')\n",
    "\n",
    "    # Fill missing 'Tip' with 'KSh 0.00' \n",
    "    df['Tip'] = df['Tip'].fillna('KSh 0.00')\n",
    "\n",
    "    df['Tip'] = df['Delivery_Charges']\n",
    "\n",
    "    df['Delivery_Charges'] = df['Discount']\n",
    "\n",
    "    df['Discount'] = df['Subtotal']\n",
    "\n",
    "    df['Subtotal'] = df['Payment_Type']\n",
    "\n",
    "    df['Payment_Type'] = df['Task_Category']\n",
    "\n",
    "    df['Task_Category'] = df['Earning']\n",
    "\n",
    "    df['Earning'] = df['Pricing']\n",
    "\n",
    "    # Remove 'YR-' and ',0' from Order_ID\n",
    "    df['Order_ID'] = df['Order_ID'].str.replace('YR-', '').str.replace(',0', '')\n",
    "\n",
    "    # Replace \"-\" values with NaN in each column\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "\n",
    "    # Drop 'Unnamed: 30' and 'Unnamed: 31' columns\n",
    "    df.drop(columns=['Unnamed: 30', 'Unnamed: 31'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"updated_data.csv\")\n",
    "\n",
    "# Define the output folder\n",
    "output_folder = \"transformed_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the output file path within the output folder\n",
    "output_file_path = os.path.join(output_folder, \"deliveries.csv\")\n",
    "\n",
    "# Update the DataFrame\n",
    "updated_df = update_data(df.copy())  # Operate on a copy\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file in the output folder\n",
    "updated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Modified data saved in '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\3974877765.py:17: DtypeWarning: Columns (7,10,18,23,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All markets data consolidated and saved in 'transformed_data\\all_markets_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the transformed data folder\n",
    "transformed_data_folder = \"transformed_data\"\n",
    "\n",
    "# List of files to read and consolidate\n",
    "files_to_read = [\"deliveries.csv\", \"customer_data.csv\", \"orders.csv\"]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Read and store DataFrames from each file\n",
    "for file_name in files_to_read:\n",
    "    file_path = os.path.join(transformed_data_folder, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes[file_name.split('.')[0]] = df\n",
    "    else:\n",
    "        print(f\"File '{file_name}' not found in '{transformed_data_folder}'.\")\n",
    "\n",
    "# Join DataFrames together\n",
    "all_markets_data = pd.concat(dataframes.values(), axis=1)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(transformed_data_folder, \"all_markets_data.csv\")\n",
    "\n",
    "# Write the consolidated DataFrame to a new CSV file\n",
    "all_markets_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All markets data consolidated and saved in '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\3857734659.py:35: DtypeWarning: Columns (7,10,18,23,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(transformed_data_folder, file_name), dtype={'ID': object})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All markets data consolidated and saved in 'transformed_data\\all_markets_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the transformed data folder\n",
    "transformed_data_folder = \"transformed_data\"\n",
    "\n",
    "# List of files to read and consolidate\n",
    "files_to_read = [\"deliveries.csv\", \"customer_data.csv\", \"orders.csv\"]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Read and store column names from each file\n",
    "column_names = {}\n",
    "for file_name in files_to_read:\n",
    "    file_path = os.path.join(transformed_data_folder, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path, nrows=0)  # Read only the header\n",
    "        column_names[file_name] = df.columns.tolist()\n",
    "    else:\n",
    "        print(f\"File '{file_name}' not found in '{transformed_data_folder}'.\")\n",
    "\n",
    "# Standardize column names and handle duplicates\n",
    "standardized_column_names = {}\n",
    "for file_name, cols in column_names.items():\n",
    "    prefix = file_name.split('.')[0] + \"_\"\n",
    "    standardized_cols = [prefix + col.lower().replace(\" \", \"_\") for col in cols]\n",
    "    standardized_column_names[file_name.split('.')[0]] = standardized_cols\n",
    "\n",
    "# Create a new DataFrame with standardized column names\n",
    "all_markets_data = pd.DataFrame()\n",
    "\n",
    "# Copy data from original files into the new DataFrame\n",
    "for file_name in files_to_read:\n",
    "    df = pd.read_csv(os.path.join(transformed_data_folder, file_name), dtype={'ID': object})\n",
    "    df.columns = standardized_column_names[file_name.split('.')[0]]\n",
    "    all_markets_data = pd.concat([all_markets_data, df], axis=1)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = os.path.join(transformed_data_folder, \"all_markets_data.csv\")\n",
    "\n",
    "# Write the consolidated DataFrame to a new CSV file\n",
    "all_markets_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All markets data consolidated and saved in '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\2823578641.py:17: DtypeWarning: Columns (6,7,10,18,23,26,31,33,34,39,40,41,50,52,55,57,58,59,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  consolidated_data = pd.read_csv(os.path.join(consolidated_data_folder, consolidated_file_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified data saved in 'data\\markets_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the directory containing the consolidated data file\n",
    "consolidated_data_folder = \"transformed_data\"\n",
    "\n",
    "# Define the file name of the consolidated data\n",
    "consolidated_file_name = \"all_markets_data.csv\"\n",
    "\n",
    "# Define the output folder for the modified data\n",
    "output_folder = \"data\"\n",
    "\n",
    "# Define the output file name\n",
    "output_file_name = \"markets_data.csv\"\n",
    "\n",
    "# Read the consolidated data file\n",
    "consolidated_data = pd.read_csv(os.path.join(consolidated_data_folder, consolidated_file_name))\n",
    "\n",
    "# Define the columns that may have decimal values\n",
    "columns_to_check = ['deliveries_order_id', 'customer_data_customer_id', 'orders_order_id', 'orders_transaction_id', 'orders_customer_id', 'orders_merchant_id']\n",
    "\n",
    "# Remove '.0' from the values in the specified columns\n",
    "for column in columns_to_check:\n",
    "    consolidated_data[column] = consolidated_data[column].astype(str).str.replace(r'\\.0', '', regex=True)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "consolidated_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Modified data saved in '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column headers of markets_data.csv:\n",
      "['deliveries_task_id', 'deliveries_order_id', 'deliveries_relationship', 'deliveries_team_name', 'deliveries_task_type', 'deliveries_notes', 'deliveries_agent_id', 'deliveries_agent_name', 'deliveries_distance(m)', 'deliveries_total_time_taken(min)', 'deliveries_task_status', 'deliveries_ref_images', 'deliveries_rating', 'deliveries_review', 'deliveries_latitude', 'deliveries_longitude', 'deliveries_tags', 'deliveries_promo_applied', 'deliveries_custom_template_id', 'deliveries_task_details_qty', 'deliveries_task_details_amount', 'deliveries_special_instructions', 'deliveries_tip', 'deliveries_delivery_charges', 'deliveries_discount', 'deliveries_subtotal', 'deliveries_payment_type', 'deliveries_task_category', 'deliveries_earning', 'deliveries_pricing', 'customer_data_customer_id', 'customer_data_last_used_platform', 'customer_data_is_blocked', 'customer_data_created_at', 'customer_data_language', 'customer_data_outstanding_amount', 'customer_data_loyalty_points', 'customer_data_number_of_employees', 'orders_order_id', 'orders_order_status', 'orders_category_name', 'orders_sku', 'orders_quantity', 'orders_unit_price', 'orders_cost_price', 'orders_total_cost_price', 'orders_total_price', 'orders_order_total', 'orders_sub_total', 'orders_remaining_balance', 'orders_payment_method', 'orders_transaction_id', 'orders_currency_symbol', 'orders_customer_id', 'orders_merchant_id', 'orders_description', 'orders_distance_(in_km)', 'orders_order_time', 'orders_pickup_time', 'orders_delivery_time', 'orders_ratings', 'orders_reviews', 'orders_order_preparation_time', 'orders_redeemed_loyalty_points', 'orders_consumed_loyalty_points', 'orders_flat_discount']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMK9\\AppData\\Local\\Temp\\ipykernel_1740\\2189261056.py:1: DtypeWarning: Columns (6,7,10,18,23,26,31,33,34,39,40,41,50,52,55,57,58,59,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/markets_data.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/markets_data.csv\")\n",
    "\n",
    "print(\"Column headers of markets_data.csv:\")\n",
    "for column in df.columns:\n",
    "    print(column)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
